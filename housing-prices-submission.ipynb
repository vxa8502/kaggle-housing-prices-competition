{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10211,"databundleVersionId":111096,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Get the Data \n\n### Setup Workspace","metadata":{}},{"cell_type":"code","source":"# Data manipulation and analysis\nimport numpy as np\nimport pandas as pd\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\n# Statistical analysis\nfrom scipy.stats import randint, uniform, loguniform\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\n# Data preprocessing and pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom category_encoders import BinaryEncoder\n\n# Model selection and evaluation\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Machine learning models\nfrom sklearn.ensemble import (\n    RandomForestRegressor,\n    GradientBoostingRegressor,\n    StackingRegressor\n)\nfrom sklearn.linear_model import Ridge\nimport xgboost as xgb\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"820a9831-2a3f-4ff5-bedb-9d9df939aa30","_cell_guid":"86af02c6-ac6c-472a-bac1-12cf53dfb335","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-01T04:47:36.763866Z","iopub.execute_input":"2025-06-01T04:47:36.764855Z","iopub.status.idle":"2025-06-01T04:47:37.511004Z","shell.execute_reply.started":"2025-06-01T04:47:36.764807Z","shell.execute_reply":"2025-06-01T04:47:37.509271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the training dataset\ntrain_original = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\", index_col='Id')\ntest_original = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv\", index_col='Id')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T04:47:37.512250Z","iopub.execute_input":"2025-06-01T04:47:37.513593Z","iopub.status.idle":"2025-06-01T04:47:37.609058Z","shell.execute_reply.started":"2025-06-01T04:47:37.513560Z","shell.execute_reply":"2025-06-01T04:47:37.607934Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Custom Transformers and Pipeline Implementation","metadata":{"_uuid":"e15034ad-466a-4996-aa02-c447c7d98596","_cell_guid":"61a0fab9-f7a2-488b-83e1-14eee4864574","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### Custom Transformers","metadata":{"_uuid":"f2cbb2e9-9b9c-4760-9904-eccd0d4c85a2","_cell_guid":"491fb323-7da7-4162-8ff9-f942883f4ca9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class DataTypeCleaner(BaseEstimator, TransformerMixin):\n    \"\"\"Convert categorical attributes masquerading as numerical to objects\"\"\"\n    \n    def __init__(self):\n        self.categorical_masqueraders = ['MSSubClass', 'OverallCond', 'OverallQual', 'MoSold']\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X_copy = X.copy()\n        for col in self.categorical_masqueraders:\n            if col in X_copy.columns:\n                X_copy[col] = X_copy[col].astype(str)\n        return X_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T04:47:37.648995Z","iopub.execute_input":"2025-06-01T04:47:37.649381Z","iopub.status.idle":"2025-06-01T04:47:37.658277Z","shell.execute_reply.started":"2025-06-01T04:47:37.649354Z","shell.execute_reply":"2025-06-01T04:47:37.657132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CompositeFeatureDropper(BaseEstimator, TransformerMixin):\n    \"\"\"Drop composite features that cause multicollinearity\"\"\"\n    \n    def __init__(self):\n        self.variables_to_drop = ['GrLivArea', 'TotalBsmtSF']\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X.drop(columns=[col for col in self.variables_to_drop if col in X.columns])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T04:47:37.659654Z","iopub.execute_input":"2025-06-01T04:47:37.660065Z","iopub.status.idle":"2025-06-01T04:47:37.687120Z","shell.execute_reply.started":"2025-06-01T04:47:37.660038Z","shell.execute_reply":"2025-06-01T04:47:37.685811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Create new engineered features\"\"\"\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        X_copy = X.copy()\n        \n        # 1. Total Bathrooms\n        X_copy[\"TotalBathrooms\"] = (\n            X_copy[\"FullBath\"] + \n            X_copy[\"HalfBath\"] * 0.5 + \n            X_copy[\"BsmtFullBath\"] + \n            X_copy[\"BsmtHalfBath\"] * 0.5\n        )\n        \n        # 2. Basement Value Index\n        X_copy['BasementValue'] = (\n            X_copy['BsmtFinSF1'] * 0.8 + \n            X_copy['BsmtFinSF2'] * 0.6 + \n            X_copy['BsmtFullBath'] * 300\n        )\n        \n        # 3. Comprehensive Garage Score (handle division by zero)\n        garage_year_normalized = X_copy['GarageYrBlt'].replace(0, 2000) / 2000\n        X_copy['GarageScore'] = (\n            X_copy['GarageCars'] * \n            X_copy['GarageArea'] * \n            garage_year_normalized\n        )\n        \n        return X_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T04:47:37.688832Z","iopub.execute_input":"2025-06-01T04:47:37.689273Z","iopub.status.idle":"2025-06-01T04:47:37.714944Z","shell.execute_reply.started":"2025-06-01T04:47:37.689235Z","shell.execute_reply":"2025-06-01T04:47:37.713602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MissingValueHandler(BaseEstimator, TransformerMixin):\n    \"\"\"Handle missing values with domain-specific logic\"\"\"\n    \n    def __init__(self):\n        self.neighborhood_medians = None\n        self.overall_median = None\n        \n    def fit(self, X, y=None):\n        # Calculate neighborhood medians for LotFrontage\n        if 'LotFrontage' in X.columns and 'Neighborhood' in X.columns:\n            self.neighborhood_medians = X.groupby('Neighborhood')['LotFrontage'].median()\n            self.overall_median = X['LotFrontage'].median()\n        return self\n    \n    def transform(self, X):\n        X_copy = X.copy()\n        \n        # 1. Drop features with high missing values\n        features_to_drop = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']\n        X_copy = X_copy.drop(columns=[col for col in features_to_drop if col in X_copy.columns])\n        \n        # 2. Handle straightforward missing values\n        X_copy['MasVnrArea'] = X_copy['MasVnrArea'].fillna(0)\n        X_copy['MasVnrType'] = X_copy['MasVnrType'].fillna('None')\n        X_copy['Electrical'] = X_copy['Electrical'].fillna('None')\n        \n        # 3. Handle LotFrontage with neighborhood-based imputation\n        if 'LotFrontage' in X_copy.columns and self.neighborhood_medians is not None:\n            def impute_lotfrontage(row):\n                if pd.isnull(row['LotFrontage']):\n                    neighborhood = row['Neighborhood']\n                    if neighborhood in self.neighborhood_medians:\n                        return self.neighborhood_medians[neighborhood]\n                    else:\n                        return self.overall_median\n                return row['LotFrontage']\n            \n            X_copy['LotFrontage'] = X_copy.apply(impute_lotfrontage, axis=1)\n        \n        # 4. Handle garage-related features\n        garage_categorical = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\n        for col in garage_categorical:\n            if col in X_copy.columns:\n                X_copy[col] = X_copy[col].fillna('None')\n        \n        garage_numerical = ['GarageCars', 'GarageArea', 'GarageYrBlt']\n        for col in garage_numerical:\n            if col in X_copy.columns:\n                X_copy[col] = X_copy[col].fillna(0)\n        \n        # 5. Handle basement-related features\n        basement_categorical = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n        for col in basement_categorical:\n            if col in X_copy.columns:\n                X_copy[col] = X_copy[col].fillna('None')\n        \n        basement_numerical = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath']\n        for col in basement_numerical:\n            if col in X_copy.columns:\n                X_copy[col] = X_copy[col].fillna(0)\n        \n        return X_copy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T04:47:37.716844Z","iopub.execute_input":"2025-06-01T04:47:37.717268Z","iopub.status.idle":"2025-06-01T04:47:37.753800Z","shell.execute_reply.started":"2025-06-01T04:47:37.717228Z","shell.execute_reply":"2025-06-01T04:47:37.752488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"Handle all categorical encoding with proper strategy\"\"\"\n    \n    def __init__(self):\n        self.binary_encoder = None\n        self.ordinal_mappings = {\n            'OverallQual': {'1': 1, '2': 2, '3': 3, '4': 4, '5': 5, \n                            '6': 6, '7': 7, '8': 8, '9': 9, '10': 10},\n            'OverallCond': {'1': 1, '2': 2, '3': 3, '4': 4, '5': 5, \n                            '6': 6, '7': 7, '8': 8, '9': 9, '10': 10},\n            'ExterQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n            'ExterCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n            'BsmtQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n            'BsmtCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n            'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4},\n            'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n            'BsmtFinType2': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6},\n            'HeatingQC': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n            'KitchenQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n            'Functional': {'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4, 'Mod': 5, 'Min2': 6, 'Min1': 7, 'Typ': 8},\n            'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3},\n            'GarageQual': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n            'GarageCond': {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n            'PavedDrive': {'N': 0, 'P': 1, 'Y': 2},\n            'Utilities': {'ELO': 1, 'NoSeWa': 2, 'NoSewr': 3, 'AllPub': 4},\n            'LandSlope': {'Sev': 1, 'Mod': 2, 'Gtl': 3},\n            'Electrical': {'None': 0, 'FuseP': 1, 'FuseF': 2, 'FuseA': 3, 'Mix': 4, 'SBrkr': 5}\n        }\n        self.label_encoders = {}\n        self.onehot_columns = None\n        # Store default values for ordinal features (most common/middle value)\n        self.ordinal_defaults = {\n            'KitchenQual': 3,  # TA (typical/average)\n            'Functional': 8,   # Typ (typical)\n            'Utilities': 4,    # AllPub (most common)\n            'HeatingQC': 3,    # TA (typical/average)\n            'ExterQual': 3,    # TA (typical/average)\n            'ExterCond': 3,    # TA (typical/average)\n        }\n        \n    def fit(self, X, y=None):\n        # Identify categorical columns\n        cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n        \n        # Define encoding strategies\n        self.high_cardinality_nominal = ['Neighborhood', 'Exterior1st', 'Exterior2nd', 'MSSubClass', 'MoSold']\n        self.high_cardinality_nominal = [col for col in self.high_cardinality_nominal if col in cat_cols]\n        \n        nominal_features = [\n            \"MSZoning\", \"Street\", \"LotShape\", \"LandContour\", \"LotConfig\", \"Neighborhood\",\n            \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\",\n            \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\",\n            \"GarageType\", \"SaleType\", \"SaleCondition\", \"MSSubClass\", \"MoSold\"\n        ]\n        \n        self.low_cardinality_nominal = [col for col in nominal_features \n                                       if col in cat_cols and col not in self.high_cardinality_nominal]\n        \n        ordinal_features = [\n            \"Utilities\", \"Electrical\", \"LandSlope\", \"OverallQual\", \"OverallCond\", \"ExterQual\",\n            \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n            \"HeatingQC\", \"KitchenQual\", \"Functional\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PavedDrive\"\n        ]\n        self.ordinal_features = [col for col in ordinal_features if col in cat_cols]\n        \n        # Fit binary encoder for high cardinality features\n        if self.high_cardinality_nominal:\n            self.binary_encoder = BinaryEncoder(cols=self.high_cardinality_nominal, return_df=True)\n            self.binary_encoder.fit(X)\n        \n        # One-hot encoding: store dummy columns from training data\n        if self.low_cardinality_nominal:\n            dummies = pd.get_dummies(X[self.low_cardinality_nominal], \n                                   prefix=self.low_cardinality_nominal,\n                                   drop_first=True)\n            self.onehot_columns = dummies.columns.tolist()\n        \n        # Fit label encoders for ordinal features without predefined mappings\n        for feature in self.ordinal_features:\n            if feature not in self.ordinal_mappings:\n                le = LabelEncoder()\n                le.fit(X[feature].astype(str))\n                self.label_encoders[feature] = le\n        \n        return self\n    \n    def transform(self, X):\n        X_copy = X.copy()\n        \n        # 1. Binary encoding for high cardinality nominal features\n        if self.binary_encoder is not None:\n            X_copy = self.binary_encoder.transform(X_copy)\n        \n        # 2. One-hot encoding for low cardinality nominal features\n        if self.low_cardinality_nominal:\n            # Create dummies for current data\n            dummies = pd.get_dummies(X_copy[self.low_cardinality_nominal], \n                                   prefix=self.low_cardinality_nominal,\n                                   drop_first=True)\n            \n            # Drop original categorical columns\n            for col in self.low_cardinality_nominal:\n                X_copy.drop(col, axis=1, inplace=True)\n            \n            # Reindex to match training columns (adds missing cols with 0, removes extra cols)\n            dummies = dummies.reindex(columns=self.onehot_columns, fill_value=0)\n            \n            # Concatenate back to main dataframe\n            X_copy = pd.concat([X_copy, dummies], axis=1)\n        \n        # 3. Ordinal encoding with proper handling of missing/unknown values\n        for feature in self.ordinal_features:\n            if feature in self.ordinal_mappings:\n                # Use predefined mapping\n                X_copy[feature] = X_copy[feature].map(self.ordinal_mappings[feature])\n                \n                # Fill any remaining NaN values with default value\n                if X_copy[feature].isnull().any():\n                    default_value = self.ordinal_defaults.get(feature, \n                                                            list(self.ordinal_mappings[feature].values())[len(self.ordinal_mappings[feature])//2])\n                    X_copy[feature] = X_copy[feature].fillna(default_value)\n            else:\n                # Use fitted label encoder\n                if feature in self.label_encoders:\n                    # Handle unknown categories by assigning them to the most frequent class\n                    le = self.label_encoders[feature]\n                    known_categories = set(le.classes_)\n                    \n                    # Transform known values\n                    mask = X_copy[feature].astype(str).isin(known_categories)\n                    X_copy.loc[mask, feature] = le.transform(X_copy.loc[mask, feature].astype(str))\n                    \n                    # Handle unknown values\n                    if not mask.all():\n                        # Use the most frequent class (first class in LabelEncoder)\n                        unknown_value = le.transform([le.classes_[0]])[0]\n                        print(f\"Warning: Found {(~mask).sum()} unknown values in {feature}, filling with {unknown_value}\")\n                        X_copy.loc[~mask, feature] = unknown_value\n        \n        return X_copy","metadata":{"_uuid":"5d66efc0-9189-4584-8715-5a5c7a57eb18","_cell_guid":"f3b11343-2fa6-40f6-aeda-a282b824c446","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-01T04:47:37.755574Z","iopub.execute_input":"2025-06-01T04:47:37.755903Z","iopub.status.idle":"2025-06-01T04:47:37.794679Z","shell.execute_reply.started":"2025-06-01T04:47:37.755882Z","shell.execute_reply":"2025-06-01T04:47:37.793359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Complete Preprocessing Pipeline","metadata":{"_uuid":"f989afc5-af7f-481c-a921-7458e00a1358","_cell_guid":"9cf064e6-8fb2-44b3-a8c6-b9041612176c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def create_preprocessing_pipeline():\n    \"\"\"Create the complete preprocessing pipeline\"\"\"\n    \n    pipeline = Pipeline([\n        ('dtype_cleaner', DataTypeCleaner()),\n        ('missing_handler', MissingValueHandler()),\n        ('feature_engineer', FeatureEngineer()),\n        ('composite_dropper', CompositeFeatureDropper()),\n        ('categorical_encoder', CategoricalEncoder())\n    ])\n    \n    return pipeline","metadata":{"_uuid":"3e7ec1b7-ebe2-435b-9513-4e1ee232fb6d","_cell_guid":"289c336d-0e61-4b48-ba15-b4ebaaf7bc75","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-01T04:47:37.798958Z","iopub.execute_input":"2025-06-01T04:47:37.799355Z","iopub.status.idle":"2025-06-01T04:47:37.826082Z","shell.execute_reply.started":"2025-06-01T04:47:37.799321Z","shell.execute_reply":"2025-06-01T04:47:37.824809Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Apply Pipeline to Training Data","metadata":{"_uuid":"316acf90-3475-4c45-839e-b17d1db88c24","_cell_guid":"22ca12b1-40a9-4c6a-83ba-b5fb9317d8c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Create and fit the preprocessing pipeline\npreprocessing_pipeline = create_preprocessing_pipeline()\n\n# Load original training data\nX_train_original = train_original.drop(['SalePrice'], axis=1)\ny_train = train_original['SalePrice']\n\n# Fit and transform training data\nX_train_processed = preprocessing_pipeline.fit_transform(X_train_original)\n\nprint(f\"Original training shape: {X_train_original.shape}\")\nprint(f\"Processed training shape: {X_train_processed.shape}\")\n\nif hasattr(X_train_processed, 'isnull'):\n    print(f\"Missing values in processed training data: {X_train_processed.isnull().sum().sum()}\")\nelse:\n    print(\"Processed training data has no null-check support (likely a NumPy array)\")","metadata":{"_uuid":"61509244-2342-488e-a693-c92fe32f10be","_cell_guid":"dac06b26-d41f-48dc-8a24-f7244b48a02c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-01T04:47:37.827438Z","iopub.execute_input":"2025-06-01T04:47:37.827768Z","iopub.status.idle":"2025-06-01T04:47:38.067364Z","shell.execute_reply.started":"2025-06-01T04:47:37.827738Z","shell.execute_reply":"2025-06-01T04:47:38.066346Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Apply Pipeline to Test Data","metadata":{"_uuid":"3fed0e75-03f0-4d81-8a06-160833951aa2","_cell_guid":"8213a28b-52dc-4123-8c76-c4b4dfc4f717","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load and process test data\nX_test_processed = preprocessing_pipeline.transform(test_original)\nprint(f\"Processed test shape: {X_test_processed.shape}\")\n\nif hasattr(X_test_processed, 'isnull'):\n    print(f\"Missing values in processed test data: {X_test_processed.isnull().sum().sum()}\")\nelse:\n    print(\"Processed test data has no null-check support (likely a NumPy array)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T04:47:38.068819Z","iopub.execute_input":"2025-06-01T04:47:38.069153Z","iopub.status.idle":"2025-06-01T04:47:38.192495Z","shell.execute_reply.started":"2025-06-01T04:47:38.069129Z","shell.execute_reply":"2025-06-01T04:47:38.190990Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Align Train and Test Features","metadata":{"_uuid":"7910ceea-3215-41ad-8602-f4efdb331a2b","_cell_guid":"112a36f6-474d-471c-a16e-bc5b52bda494","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Ensure both datasets have the same columns\ntrain_cols = set(X_train_processed.columns)\ntest_cols = set(X_test_processed.columns)\n\n# Columns in train but not in test\nmissing_in_test = train_cols - test_cols\nif missing_in_test:\n    print(f\"Adding missing columns to test set: {missing_in_test}\")\n    for col in missing_in_test:\n        X_test_processed[col] = 0  # Add with default value\n\n# Columns in test but not in train\nextra_in_test = test_cols - train_cols\nif extra_in_test:\n    print(f\"Removing extra columns from test set: {extra_in_test}\")\n    X_test_processed = X_test_processed.drop(columns=list(extra_in_test))\n\n# Reorder test columns to match train\nX_test_processed = X_test_processed[X_train_processed.columns]\n\nprint(f\"Final aligned shapes - Train: {X_train_processed.shape}, Test: {X_test_processed.shape}\")","metadata":{"_uuid":"0675769d-9f78-4e3a-ad98-853151b98db1","_cell_guid":"45815d01-e759-4d67-b478-512333644b8d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-01T04:47:38.194088Z","iopub.execute_input":"2025-06-01T04:47:38.194440Z","iopub.status.idle":"2025-06-01T04:47:38.206558Z","shell.execute_reply.started":"2025-06-01T04:47:38.194408Z","shell.execute_reply":"2025-06-01T04:47:38.204865Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train Final Model and Make Predictions","metadata":{"_uuid":"4c916a63-02e5-428d-987b-465a903f4f41","_cell_guid":"2010cae5-abf5-410e-8620-261c5cf4af24","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### Define Best Models from Analysis","metadata":{}},{"cell_type":"code","source":"# Best GradientBoosting model from hyperparameter tuning\nbest_gb = GradientBoostingRegressor(\n    learning_rate=0.04210216968916263,\n    max_depth=7,\n    max_features='log2',\n    min_samples_leaf=2,\n    min_samples_split=10,\n    n_estimators=295,\n    subsample=0.8771561434767757,\n    random_state=42\n)\n\n# Best XGBoost model from hyperparameter tuning\nbest_xgb = xgb.XGBRegressor(\n    colsample_bytree=0.7553073535959489,\n    gamma=6.841604512504889e-07,\n    learning_rate=0.004557854806921613,\n    max_depth=7,\n    min_child_weight=9,\n    n_estimators=839,\n    reg_alpha=7.5144413702907125,\n    reg_lambda=0.019206818224206303,\n    subsample=0.8144385465376481,\n    random_state=42,\n    n_jobs=-1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T04:47:38.234254Z","iopub.execute_input":"2025-06-01T04:47:38.235005Z","iopub.status.idle":"2025-06-01T04:47:38.264466Z","shell.execute_reply.started":"2025-06-01T04:47:38.234978Z","shell.execute_reply":"2025-06-01T04:47:38.262903Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create Best Ensemble Model (Stacking with Ridge)","metadata":{}},{"cell_type":"code","source":"# Create the best performing ensemble: Stacking with Ridge\nfinal_ensemble = StackingRegressor(\n    estimators=[\n        ('gb', best_gb),\n        ('xgb', best_xgb)\n    ],\n    final_estimator=Ridge(alpha=1.0),\n    cv=5,\n    n_jobs=-1\n)\n\nprint(\"Created Stacking Ensemble with Ridge meta-learner\")\nprint(\"Base models: GradientBoosting + XGBoost\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T04:47:38.266591Z","iopub.execute_input":"2025-06-01T04:47:38.267003Z","iopub.status.idle":"2025-06-01T04:47:38.294218Z","shell.execute_reply.started":"2025-06-01T04:47:38.266973Z","shell.execute_reply":"2025-06-01T04:47:38.293119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Cross-Validation of Final Model","metadata":{}},{"cell_type":"code","source":"# Perform cross-validation to confirm performance\ncv_folds = KFold(n_splits=5, shuffle=True, random_state=42)\n\nprint(\"Performing cross-validation on final ensemble...\")\ncv_scores = cross_val_score(\n    final_ensemble, X_train_processed, y_train,\n    cv=cv_folds, scoring='neg_mean_squared_error', n_jobs=-1\n)\n\nrmse_scores = np.sqrt(-cv_scores)\nprint(f\"\\nFinal Ensemble Cross-Validation Results:\")\nprint(f\"RMSE: {rmse_scores.mean():.0f} (+/- {rmse_scores.std() * 2:.0f})\")\nprint(f\"Individual folds: {[f'{score:.0f}' for score in rmse_scores]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T04:47:38.295218Z","iopub.execute_input":"2025-06-01T04:47:38.295487Z","iopub.status.idle":"2025-06-01T04:48:32.761176Z","shell.execute_reply.started":"2025-06-01T04:47:38.295465Z","shell.execute_reply":"2025-06-01T04:48:32.759936Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train Final Model on Full Dataset","metadata":{}},{"cell_type":"code","source":"print(\"\\nTraining final ensemble on complete training dataset...\")\nfinal_ensemble.fit(X_train_processed, y_train)\n\n# Make predictions on test set\nprint(\"Making predictions on test set...\")\ntest_predictions = final_ensemble.predict(X_test_processed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T04:48:32.762551Z","iopub.execute_input":"2025-06-01T04:48:32.762858Z","iopub.status.idle":"2025-06-01T04:48:46.349475Z","shell.execute_reply.started":"2025-06-01T04:48:32.762835Z","shell.execute_reply":"2025-06-01T04:48:46.347694Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Create Submission File","metadata":{"_uuid":"996b56b4-fefc-42eb-9001-5d06e0ff3546","_cell_guid":"5acb5a25-49aa-4949-b994-9377f585220f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Create submission dataframe\nsubmission = pd.DataFrame({\n    'Id': test_original.index,\n    'SalePrice': test_predictions\n})\n\n# Save submission file\nsubmission.to_csv('housing_price_predictions.csv', index=False)","metadata":{"_uuid":"1e1c760b-43a8-48ba-9bf3-79dd466e53cf","_cell_guid":"25ff9dc3-c86c-41eb-9156-6f81e9d530f3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-01T04:49:43.461105Z","iopub.execute_input":"2025-06-01T04:49:43.461525Z","iopub.status.idle":"2025-06-01T04:49:43.483371Z","shell.execute_reply.started":"2025-06-01T04:49:43.461497Z","shell.execute_reply":"2025-06-01T04:49:43.481855Z"}},"outputs":[],"execution_count":null}]}